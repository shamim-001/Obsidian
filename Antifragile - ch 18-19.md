[![Shortform App](https://www.shortform.com/img/logo-dark.70c1b072.svg)](https://www.shortform.com/app)

[Discover](https://www.shortform.com/app)

[Books](https://www.shortform.com/app/books)

[Articles](https://www.shortform.com/app/articles)

[My library](https://www.shortform.com/app/library)

[Search](https://www.shortform.com/app/search)

# Antifragile

[Back to Search](https://www.shortform.com/app/search)

## Chapters 18-19: Exponential Benefit or Harm

In Chapter 18, we start with a graphical representation of fragility and antifragility. **Using that simple illustration as a guide, we revisit exactly why fragile systems hate random events while antifragile systems love them.**

After that, we return to the discussion of how size causes fragility, now with an added dimension: concentration. A centralized system is much more fragile than a decentralized one, even if they add up to the same size. For example, a single large bank is more vulnerable to mistakes and bad deals than 10 banks that are each a 10th the size; the simple reason is that the large bank has more resources, and therefore has more to lose.

**We’ll also touch on the idea that large, influential systems can cause damage even outside of themselves.** When that large bank got itself into trouble, the global stock market took close to a 10% hit—one of the 10 hypothetical smaller banks doing something similar would have caused a much smaller shock to the market, if any at all.

Chapter 19 expands on the fragility of size, and how large systems like banks cause harm to those who rely on them. It also explores how we could mitigate the damage by dividing up our investments and our consumption. For example, large tuna fisheries cause harm to the environment by overfishing, but they only do so because people keep demanding tuna. If we returned to a more natural method of consuming what’s readily available, our large systems wouldn’t cause as much damage.

### Concavity and Convexity

Both fragility and antifragility have exponential effects. **In other words, as the significance of an event increases, the effect of that event increases even faster.** For example, if you punched a window you could easily break it; however, you could drum your fingers on the glass all day without damaging it. The thousands of tiny impacts from your fingers wouldn’t add up to the same effect as one large impact from your fist.

As we’ve previously discussed, when those significant events have negative effects, the situation is fragile. When they have positive effects, the situation is antifragile. We can roughly sketch this general concept as seen below:

![antifragile_graph.png](https://media.shortform.com/images/antifragile_graph.png)

A fragile situation has a limit to how good an outcome can be, but no limit (or almost no limit) to how bad an outcome can be. A graph of the situation has a _concave_ shape: it flexes outward in the middle. An antifragile situation is exactly the opposite, and the graph has a _convex_ shape: it flexes inward in the middle. **An easy way to remember this is that fragility makes a frown, and antifragility makes a smile.**

The two graphs also illustrate why fragility dislikes randomness, and antifragility loves it. Imagine picking random points on each of the graphs; depending on where the point falls on the _significance_ axis, it may have a positive or negative _outcome_. Now imagine that you keep picking such random points over and over again. Eventually you’re going to land on a point with enough significance that the outcome is either hugely negative (for the fragility graph) or hugely positive (for the antifragility graph).

A side note: Any line on a graph can be represented by an equation. Putting a negative sign in front of that equation will result in the same graph, but upside-down. This also holds true for our graphs of fragility and antifragility; the opposite of concavity is convexity. Fat Tony made his fortune in the oil slump by putting a minus sign in front of the banks’ equation, so that whenever they lost a dollar, he made one.

### Acceleration of Harm or Benefit

An intrinsic property of concave and convex graphs is that, the farther along the horizontal axis you go, the steeper the slope becomes. In other words, the outcome changes by greater amounts over the same horizontal distance.

This gives us an easy way to check systems for fragility (or antifragility). Take, for example, travel time from point A to point B on the thruway. If there’s no traffic, you’ll drive from A to B quickly and easily. Now let’s say, hypothetically, you note that when there are 10,000 cars on the thruway, travel time increases by 10 minutes. If traffic increases by _another_ 10,000 cars, your travel time now increases by 30 minutes. Add another 10,000 and you may be stuck in traffic for hours.

**Though the increases in traffic are the same, the increases in travel time get bigger and bigger.** Since the change is undesirable, we would say that the degree of harm is accelerating. This is a fragile system. If more traffic meant you somehow got to your destination sooner, then it would be an antifragile system; it would have accelerating benefits as the amount of cars on the road increases.

We’ve talked before about the fragility of relying on forecasting. Whether in finances, weather, votes, or anything else, trying to predict the future is notoriously unreliable. However, these forecast models could be made a great deal more accurate—and the decisions based on them made much less fragile—by subjecting them to a simple acceleration of harm test.

In short, take the model and ask, “What if it’s wrong?” Change some of the key assumptions by small increments and see what happens to the results. **If the positive changes outpace the negative ones, then you’ve got a model for an antifragile system.**

#### Beware of Averages

**One key thing that many models get wrong is that they rely on averages.** The problem, in light of the acceleration of harm effect, is that averages don’t take devastating extremes into account.

For example, imagine you booked a hotel room that’s kept at an average of 70° Fahrenheit. No doubt that sounds pretty reasonable. However, it could be that the room is 0° half the time, and 140° the other half. The average temperature is still 70° but, far from being comfortable, the room is downright dangerous. **The variability is more important than the average.**

Finally, imagine that there was a third graph with a straight line; for every “unit” that the significance of an event goes up by, the effect also goes up by one “unit.” This would represent an “average” situation, as hypothesized by any number of predictive models. If you were running a business, for example, your sales model might predict that more sales equals more money in a linear fashion, represented by such a graph.

First of all, that model is probably extremely inaccurate. Does it account for bulk buying and production? Supply shortages? In short, are there extraordinary benefits or downsides to selling much more product than usual?

**However, aside from that, it’s plain to see that a convex model would quickly outpace a linear one.** An antifragile system will, in the long term, perform better than the hypothetical average. Similarly, sooner or later a fragile system will fall victim to random chance or unforeseen events, and it would underperform.

### Size and Squeeze

We’ve said before that fragility increases as a system gets bigger. That’s because a larger system naturally has more resources available, but it also needs and uses more of them. The end result is that larger systems can go farther down the “significance” axis on the above graphs, and therefore encounter larger effects from random events.

For example, in January 2008, a Parisian bank called Société Générale realized that one of its employees had been playing in the stock market with enormous sums of money and hiding that fact from their systems. As a result, they had to rush to sell an incredible amount of stock—about $70 billion worth—and took billions of dollars in losses on the sales. At the same time, stock markets worldwide dropped by almost 10% as a result of the sudden event.

A smaller bank in a similar situation would have had much less of an impact, both on itself and on the world market. Imagine a bank that’s a 10th the size of Société Générale doing a similar emergency sell-off: $7 billion worth of stock and losses measured in millions rather than billions. It still wouldn’t be a good situation for the bank, of course, but the losses would be much smaller, and the effect on the global stock market would be much less—perhaps there wouldn’t even be one, as the market can absorb shocks of that size quite easily.

Aside from the fragility of size, the example of Société Générale shows one other important concept: _squeeze_. **Squeeze is when you’re forced into a bad situation because it’s the only option available.** The bank had no choice but to sell off those stocks that it didn’t know it had; its records would be egregiously wrong otherwise.

Squeeze is another type of unexpected event that reveals fragility in a system. For the reasons explained above, larger systems are also more vulnerable to squeeze—simply put, they need more resources and have more to lose than smaller systems.

Think about an elephant during a drought. The elephant needs a great deal of food and water to survive, both of which are in short supply. It’s forced to starve, possibly to death—that’s the squeeze. With no food to be had, there’s no other option available.

Now imagine a mouse in the same situation. The mouse needs a tiny fraction of the food and water that the elephant does, so it’s much less likely to be squeezed by the drought.

#### The Concentration Effect

**One way to offset the size and squeeze effects is to decentralize or defocus.** As we mentioned, a bank one 10th the size of Société Générale wouldn’t have been as vulnerable to the unexpected event of a backroom trader gone rogue. So, if the bank were to split up into 10 smaller banks, the system as a whole would be much more stable.

Similar effects are seen in major projects, ranging from construction to military (and no doubt many other areas as well). Construction projects become disproportionately more expensive as the size of the project increases. However, if the project can be broken up into many smaller tasks, managed individually, that exponential cost increase doesn’t happen.

For example, building a tunnel through a mountain is a single, large-scale project—there’s no way around it. Unexpected delays or problems hugely increase the cost, because now you have lots of people who need to be paid while the project isn’t progressing, increased supply and maintenance costs, and so on. However, repairing a city’s roads is something that can be easily broken up and assigned out to various project managers. If one manager’s project runs into problems, it’s a relatively small cost increase and doesn’t hold up progress anywhere else.

One of the most dramatic modern examples of runaway costs is the U.S. invasion of Iraq under George W. Bush. It was expected to cost somewhere between $30 billion and $60 billion—now the price tag has risen over $2 trillion. The enormous size and complexity of the U.S. military leads to chain reactions of increasing cost, compounding each other almost without limit.

**Interestingly, excessively large systems can also cause fragility—or, more simply, damage—to other systems.** For example, in our modern lifestyle, people tend to get more or less the same things every time they go shopping. On top of individuals repeatedly buying the same things, many individuals will purchase the same “staples” as everyone else; things like bread, milk, and butter.

However, it’s well known that overconsumption of a particular resource—say, tuna fish—can cause harm to the environment. Overfishing tuna disrupts the ecosystem. Other animals that prey on tuna have their food source depleted, while the organisms that tuna eat suddenly have explosive population growth.

This problem didn’t exist for our hunter-gatherer ancestors, and examining what records we have of them shows why. In simple terms, they weren’t as picky as we are today; they would eat whatever was available. **If they overhunted a particular animal and had trouble finding more of it, they’d simply switch to hunting something else.** This would naturally let the formerly hunted population recover, while keeping down the numbers of whatever animals were benefiting from its absence.

[

Previous

Chapters 16-17: How to Learn

](https://www.shortform.com/app/book/antifragile/chapters-16-17)

[

Next

Chapter 20: The Fragility of the New

](https://www.shortform.com/app/book/antifragile/chapter-20)