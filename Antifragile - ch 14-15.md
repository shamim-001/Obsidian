[![Shortform App](https://www.shortform.com/img/logo-dark.70c1b072.svg)](https://www.shortform.com/app)

[Discover](https://www.shortform.com/app)

[Books](https://www.shortform.com/app/books)

[Articles](https://www.shortform.com/app/articles)

[My library](https://www.shortform.com/app/library)

[Search](https://www.shortform.com/app/search)

# Antifragile

[Back to Search](https://www.shortform.com/app/search)

## Chapters 14-15: Practicing and Tinkering

These two chapters cover a wide range of topics, and they go into more detail about some that have already been touched upon in earlier chapters. Chapter 14 continues the discussion of epiphenomena. **It focuses especially on the false belief that formal education leads to practical skills and economic prosperity.**

Next, Chapter 15 offers several rules of thumb for finding and leveraging antifragile situations.

1. Look for situations that have a lot of options available.
2. Look for opportunities where the payoff is theoretically limitless, rather than “safe” but small.
3. Don’t back particular ideas or opportunities; back people who are adaptable and able to take advantage of options, rather than bound by rigid theories or focused on a specific outcome.
4. Remember the barbell model: Split your resources so that some are absolutely protected, and the rest are chasing those theoretically limitless payoffs.

### The Education Fallacy

**One popular epiphenomenon is that widespread formal education boosts the economy.** Strong economies and formal education tend to be seen together, so some people draw the conclusion that education leads to prosperity. However, it’s the other way around: Rich countries tend to institute formal education, not become rich _because of_ formal education.

There are, of course, many good reasons to educate the populace. Education tends to reduce income inequality, to instill good values into people, and to make them more interesting conversation partners. However, to say that these things cause economic growth is a fallacy.

**It’s also a fallacy to think that formal education makes people skilled in practice.** Author Taleb relates a time when he was fresh out of college and entering the professional world for the first time. He’d studied risk and probability, and he was, at the time, focused on exchange rates between currencies.

Coming from a polished Ivy League environment, Taleb was shocked by the money changers. Far from the refined, educated, politically aware people he expected, they were (to use Taleb’s words), “Street. Very street.” Many of them spoke English with so much slang or such heavy accents that it was barely recognizable as English. A man introduced as one of the biggest traders of Swiss francs in the world didn’t know the first thing about Switzerland.

**Taleb recalls feeling his formal education vanishing in front of his eyes—a sign of its fragility.** He’d been trained to think that knowledge and education were crucial for success, but these uneducated, barely literate men were handling enormous sums of money with ease.

#### How Fat Tony Got Fat

Now recall Fat Tony from Chapter 9. He didn’t get rich from any fancy formal education, but from the simple understanding that sooner or later, the economy would take a hit—in other words, by betting on fragility.

Specifically, he made his fortune in 1991, when the U.S. attacked Iraq near the end of the Gulf War. Economists, analysts, and journalists were all predicting that the price of oil would rise if the U.S. went to war. However, Tony bet the other way. He reasoned that, if everyone was predicting that the price of oil would go up, then the market must have already adjusted for that. Therefore, a _sudden_ war would indeed raise the price of oil, but not a _planned_ war.

Tony’s prediction turned out to be right. Rather than skyrocketing, the price of oil dropped by half, and Tony turned a $300,000 investment into $18 million. When asked how he’d known, he simply responded that war and oil were not the same thing. **In other words, he avoided the epiphenomenon that war causes oil prices to rise.**

So there were two parts to Fat Tony’s good fortune, and neither had anything to do with education or intelligence. First was his innate understanding that people are suckers, and economics are fragile. Second was the rationality to apply that understanding to current events. With everyone hoarding oil, waiting for the expected price boom, they were suckering themselves; there was suddenly much more supply than demand, and the price crashed instead.

### Not the Same Thing

**Fat Tony’s lesson that war and oil aren’t the same thing can be generalized to fit many situations.** In the most general sense, theory isn’t the same thing as practice, and the two shouldn’t be conflated. The _theory_ was that oil’s price would go up in the event of a war; in _practice_, it plummeted.

Put another way, reality and narratives aren’t the same things. Reality is what happens, and narratives are the stories we tell ourselves after the fact. In reality, rising oil prices and wars often happen at the same time; the narrative became that war _caused_ the increased oil prices, and it always would.

**However you look at it, the main difference is optionality—thus antifragility.** Theories and narratives are fragile, and very easily destroyed by information that doesn’t agree with them. Worse, they make the ones who believe in them fragile, as seen in the oil situation. Reality and practices, on the other hand, are antifragile. They develop based on a sort of natural evolution, as the following anecdote will show.

The economist Ariel Rubinstein tells a story about when he learned the difference between theory and practice. He was in the Middle East, in the area that was then called the Levant, and was trying to haggle with a vendor in the marketplace. Rubinstein tried to get the vendor to haggle based on _game theory_, a field of mathematics that’s based on interactions between purely rational parties and practiced through thought experiments and computer simulations.

The vendor played along for a little while, but Rubinstein’s experiment didn’t get them to an agreement that was acceptable for either party. The vendor then questioned why Rubinstein thought he could change the methods that merchants had developed over generations. Rubinstein was embarrassed, and he left.

The merchant’s haggling methods were based on generations of natural evolution, keeping what works and discarding what doesn’t. Recall that evolution (biological or otherwise) also tends to build in redundancies and fail-safes—even something that works well _most_ of the time will eventually be squeezed out of the population, unless it’s backed up by something else for those times when it doesn’t.

Rubinstein’s game theory could only proceed in certain ways—it was rigid and fragile. However, the merchant had many different options to fall back on if things weren’t going the way he hoped—his sales techniques were flexible and antifragile. **For all of Rubinstein’s formal education, his theories couldn’t hold up against the merchant’s practice.**

### History Told by Losers

If fragile theories and narratives don’t hold up against antifragile practices, one might wonder why the theories and narratives are still so prevalent. However, it’s natural that the ones who like to write theories and tell stories are the ones best able to spread their ideas, much more so than those who practice a trade or craft.

Academics like to claim that they create theories, which people then put into practice. **The truth is a bit more complicated, and it goes in the opposite direction.** Practitioners perfect a craft through countless generations of trial and error-style evolution, then academics create theories based on it. Those academics then turn around and claim that the practitioners are using their theories.

We can see evidence of this in accounts going as far back as the Elizabethan era of the 1500s and 1600s. There were two competing schools of medical practitioners back then—although “school” may be the wrong word for one of those, since it consisted of practitioners who learned how to treat people simply through trial and error. The more organized, more formally educated doctors had any number of names for such practitioners: quacks, charlatans, and so on.

However, these “quacks” had a great deal of support among the common folk. Naturally, the trained doctors didn’t appreciate the intrusion into their business, and so they used the weight of their education and titles to denounce the ones who didn’t share their training and theoretical knowledge. **It’s worth noting that the academics didn’t have significantly better outcomes than the ones they called charlatans.** They were simply the ones with the authority—and the mentality—to control the discourse and write the medical books.

The problems begin when the next generation of practitioners believes the academics, and bases their practice on (inevitably flawed) theories. When this happens in an important field—say, stock trading—it leads to rigidity and fragility in the practice that ends up damaging the whole system, perhaps even causing a recession. **In short, theories aren’t put _into_ practice, they grow _from_ practice.**

#### Theory Follows Practice

Any number of supposedly scientific topics show, on a closer look, that they developed from trial and error rather than theories building upon theories. For instance, people building jet engines needed the original engineers present because they didn’t have a solid theory of how the things worked until much later. Many people believe that precise mathematics is responsible for great works of architecture like ancient cathedrals, but the architects relied almost entirely on their tools and some rules of thumb.

**Most “scientific” pursuits look more like cooking than physics.** Cooking might be the perfect example of a craft that evolved naturally, based on optionality. If you like an ingredient, you keep it; if not, you don’t use it in that recipe anymore. It’s truly antifragile because every mistake makes future efforts better. We have thousands of years of cooking history—mistakes and improvements—worked into our cultures. Finally, at almost no point does pure theory enter into cooking. You can’t invent an entire new dish based on theories about ingredients you’ve never used.

There are some exceptions to the “cooking” model of science. Physics, and its offshoots like astronomy, are often advanced through pure theory. The astronomer Le Verrier determined that Neptune existed long before anyone observed it, simply based on the behavior of the astrological bodies around it. Similarly, the Higgs Boson particle was believed to exist because it _had_ to exist—our understanding of physics simply wouldn’t work without it.

However, note that these are _exceptions_. **In general, science advances through mistakes and improvements, much like an old family recipe.**

### Invest in Tinkering Rather Than Research

**The main mistake that investors, particularly government investors, make is looking for a particular outcome.** They start with a goal in mind and try to put money toward reaching it, rather than supporting the tinkering that’s led to so many unexpected breakthroughs.

Consider many of the advancements that led to the Industrial Revolution. For example, the flying shuttle revolutionized the textile industry, but it wasn’t invented by a scientist working with mechanical theories in a sterile lab. It was invented by John Kay, a craftsman looking to improve the productivity of his factory.

Similarly, the power loom was invented by Reverend Edmund Cartwright—neither a scientist nor a craftsman, merely a tinkering hobbyist. In fact, a surprising number of clergymen have made their marks on the sciences: Reverend George Garrett helped pioneer submarine design, while Reverend Jack Russell bred the hunting dog that shares his name.

Time and time again, it’s hobbyists and tinkerers who invent and discover exceptional things. In spite of that fact, governments insist on funding scientific research. Even worse, they fund scientific research aimed at a particular goal.

There are _some_ examples of this method getting the desired results—the Space Race being one notable example—but on the whole this outcome-focused investment has a terrible rate of return.

#### A Case Study: Medical Research

The U.S. provides a perfect illustration of the failures of results-focused research. In the early 1970s, President Nixon declared a “war on cancer.” Morton Myers, a doctor and researcher, gives one account of the results in his book _Happy Accidents_.

Myers writes that over 144,000 plant extracts were screened and tested over two decades. **During that time, not even one plant-based cancer drug was approved by the FDA.** In contrast, an important group of anticancer drugs called the Vinca Alkaloids were discovered in the 1950s purely by chance. Going even further back, doctors discovered the potential of chemotherapy by examining soldiers who had been exposed to mustard gas—the chemical agent had a noticeable effect on soldiers suffering from various blood cancers.

John LaMatina, a pharmaceutical industry insider, published statistics showing that nine out of 10 drugs were developed by private industry. The National Institutes of Health, which is tax-funded, found that less than 10 percent of drugs on the market were developed with government funding.

**Time after time, we see that government-funded, outcome-focused research fails to give the results that privatized tinkering delivers.**

### Fund Good People, Not Good Ideas

The reason for this goes back to the previous discussion about theory versus practice. By its nature, tinkering has a lot of options available at any time, and the tinkerer can pursue whichever one looks most promising. However, if people are trying to work toward a particular outcome, their methods tend to be rigid and theory-based—in other words, fragile. If their theory doesn’t pan out, then all the work they put into it was for nothing.

Especially notable is the fact that, as our theoretical understanding of medicine increased, the number of new drugs coming out actually _decreased_. By using theoretical knowledge to pursue specific results, scientists and researchers made themselves less effective in developing and discovering new treatments.

**Therefore, rather than backing ideas that sound good, one should back good people.** Invest in the type of people who never stop tinkering, who stumble onto good ideas and then recognize them for what they are. Better yet, invest in a lot of people like that. This operates on the same theory as stock market investments from Chapter 11: A little money invested in a lot of places will have a much better rate of return than a lot of money invested in a few “safe” options.

**The reason is, all you need is for one of those inventors to hit upon something great, and you’ll make massive returns on your small investment.** You might lose a small amount of money from other investments, but you don’t want to miss betting on the winning horse, as it were. You may not end up funding the cure for cancer, but who’s to say you won’t happen to invest in the next Viagra?

#### Inverting the “Turkey Problem”

Recall the so-called Turkey Problem from Chapter 5. In short, the farm-raised turkey believes that it’s being well cared for and doesn’t foresee anything changing. It bases its life decisions on that idea. Every day that passes strengthens the turkey’s belief that it’s in a good situation, right up until the day when the farmer kills it for meat. **The turkey’s problem is that it doesn’t consider that there may be an unseen, rarely encountered, but devastating downside to its situation.**

Fragile situations like the turkey’s tend to have all their positive attributes on full display, and they hide the enormous downsides. Though not quite as dramatic as a turkey having its neck wrung, this is also the situation with results-focused research: The potential to develop a new life-saving drug is enticing, but it’s also possible to dump nearly limitless money into the search and have nothing to show for it.

Now, what would the opposite of the turkey’s situation be?—something that looks unappealing, but has the potential for almost unlimited gains. This is how many antifragile things look: long chains of errors and failures, but once in a while there’ll be a significant return on investment, as with the chance discovery of the Vinca Alkaloids.

Remember the key difference between fragility and antifragility: For fragile things, random events are a net negative. They have more to lose than to gain from rare or unforeseen events. For antifragile things, it’s the opposite: They have more to gain than to lose, so random events are a net positive. **Therefore, even though the fragile situation seems more appealing—and may look that way for quite a long time—in the long run, the antifragile situation is more desirable.**

[

Previous

Chapters 12-13: Optionality and Rationality

](https://www.shortform.com/app/book/antifragile/chapters-12-13)

[

Next

Exercise: Think About Tinkering

](https://www.shortform.com/app/book/antifragile/exercise-think-about-tinkering)