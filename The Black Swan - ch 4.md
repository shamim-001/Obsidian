## Chapter 4: The Scandal of Prediction

With the rapid advance of technology—computer chips, cellular networks, the Internet—it stands to reason that our predictive capabilities too are advancing. But consider how few of these groundbreaking advances in technology were _themselves_ predicted. For example, no one predicted the Internet, and it was more or less ignored when it was created.

(Shortform note: It’s unclear how Taleb defines “predicted.” Plenty of science-fiction writers and cultural commentators anticipated recent technologies like the Internet and augmented and virtual reality.)

It is an inconvenient truth that humans’ predictive capabilities are extremely limited; we are continuously faced with catastrophic or revolutionary events that arrive completely unexpectedly and for which we have no plan. Yet, nevertheless, we maintain that the future is knowable and that we can adequately prepare for it. Taleb calls this tendency the _scandal of prediction_.

### Epistemic Arrogance

The reason we overestimate our ability to predict is that we’re overconfident in our knowledge.

A classic illustration of the fact comes from a study conducted by a pair of Harvard researchers. In the study, the researchers asked subjects to answer specific questions with numerical ranges. (A sample question might be, “How many redwoods are there in Redwood Park in California?” To which the subject would respond, “I’m 98% sure there are between _x_ and _y_ number of redwoods.) The researchers found that the subjects, though they were 98% sure of their answers, ended up being wrong 45% of the time! (Fun fact: The subjects of the study were Harvard MBAs.) In other words, the subjects picked overly narrow ranges because they _overestimated they own ability to estimate_. If they had picked wider ranges—and, in so doing, acknowledged their own lack of knowledge—they would have scored much better.

Taleb calls our overconfidence in our knowledge “epistemic arrogance.” On the one hand, we overestimate what we know; on the other, we underestimate what we don’t—_uncertainty_.

It’s important to recognize that Taleb isn’t talking about how much or how little we actually know, but rather the disparity between what we know and what we _think_ we know. **_We’re arrogant because we think we know more than we actually do_**.

This arrogance leads us to draw a distinction between “guessing” and “predicting.” Guessing is when we attempt to fill in a _nonrandom_ variable based on incomplete information, whereas predicting is attempting to fill in a _random_ variable based on incomplete information.

Say, for example, someone asks you to estimate how many natural lakes there are in Georgia. There’s a right answer to the question—it’s 0—but you don’t know it, so your answer is a “guess.”

But say that same someone asks you what the U.S. unemployment rate will be in a year. You might look at past figures, GDP growth, and other metrics to try and make a “prediction.” **But the fact is, your answer will still be a “guess”**—there are just too many factors (unknown unknowns) to venture anything better than a guess.

#### The Curse of Information

It stands to reason that the greater our information is about a particular problem, the more likely we are to come upon a solution. And the same goes, it would seem, for predictions: The more information we have to _make_ a prediction, the more accurate our prediction will be.

But an array of studies shows that **an increase in information actually has negligible—and even _negative_—effects on our predictions**.

For example, the psychologist Paul Slovic conducted a study on oddsmakers at horse tracks. He had the oddsmakers pick the ten most important variables for making odds, then asked the oddsmakers to create odds for a series of races using only those variables.

In the second part of the experiment, Slovic gave the oddsmakers ten _more_ variables and asked them to predict again. **The accuracy of their predictions was the same** (though their _confidence_ in their predictions increased significantly).

**The negative outcome of an increase in information is that we become increasingly sure of our predictions even as their accuracy remains constant**.

#### Experts—The Worst Offenders

The most arrogant group in terms of their predictions—and least aware of their own ignorance—are so-called “experts.” These are the credentialed and/or laureled people whose opinions are granted weight by society.

Taleb divides this group by two. There are those who are arrogant but also display some degree of competence, and then there are those who are arrogant _and_ incompetent.

**1) Competent Arrogants**

“Competent Arrogants” are those experts with actual predictive abilities and discernible skills. This group includes **astronomers, physicists, surgeons, and mathematicians** (when dealing exclusively with pure, rather than applied, mathematics).

**2) Incompetent Arrogants**

“Incompetent Arrogants” are “experts” whose predictive abilities and skills aren’t significantly greater than the average person’s. This group includes **stockbrokers, intelligence analysts, clinical psychologists, psychiatrists, economists, finance professors, and personal financial advisors**.

In numerous empirical studies, the forecasting ability of the “incompetent arrogants” has been shown to be almost nonexistent.

For example, over the course of several years, psychologist Philip Tetlock asked almost 300 specialists—political scientists, economists, journalists, and politicians—to offer predictions of world events (the timeframe was usually “within the next five years”). **He discovered that the experts’ predictions were barely more accurate than random selection and often worse than simple computer simulations**.

He also found that the more prominent a person was in his or her field, the worse were his or her predictions_._ The reason for this finding was that prominent people tend to become prominent based on their having _one big idea_. These experts marry themselves to their singular idea and neglect other possibilities—and thus, when randomness rears its head, they’re shown to have been woefully misguided.

Tetlock’s main interest, however, wasn’t the fact of experts’ poor forecasting abilities but rather those experts’ _lack of accountability_ for being wrong. He found that experts (unconsciously) employ a number of excuses to explain away their errors.

**The “Different Game” Defense**

Experts will claim that the event that disproved their prediction could have been predicted if the right data were available. For example, a specialist in the Soviet Union who failed to predict its collapse might say that, because the Soviet Union was so adept at hiding its economic data, he wasn’t able to make an accurate prediction.

**The “Almost Right” Defense**

Experts will claim that, if a minor variable or two had been different, they would have been proven correct. For example, a specialist in the Soviet Union who predicted that it would collapse twenty years after it actually did might say something like, “Well, I knew it _would_ collapse, I just didn’t know exactly _when_.”

**The “Outlier” Defense**

Experts will claim that the event they failed to predict was a complete anomaly, a thousand-year flood—a Black Swan. For example, a specialist in the Soviet Union will claim its collapse _couldn’t_ have been predicted, and so her inability to predict it doesn’t mean her predictive ability is suspect.

### Problems with Projections

Like other predictions, projections—of incomes, costs, price fluctuations, construction time, and the like—are notoriously inaccurate.

This is because the authors of projections, like the authors of other kinds of predictions, “tunnel”—that is, they exclude from their calculations events external to whatever method they’re using.

Taleb cites the example of oil prices. In 1970, U.S. officials projected that the price of a barrel of oil would remain static or decline over the next ten years. In fact, due to crises like the Yom Kippur War and the Iranian Revolution, crude oil prices rose _tenfold_ by 1980. (Shortform note: One can imagine those U.S. officials employing the “Different Game” and “Outlier” defenses to explain why their projection proved incorrect.)

A major problem with financial and governmental projections is that **they don’t incorporate a margin of error**. That is, the authors of these projections don’t take into account (nor do they publicize) how significantly their projections might be off-target.

The disregard for and omission of margins of error reveal three fallacies of projection:

1) **The “Final Number” Fallacy**

Because corporate projections omit a margin of error, we tend to fixate on the final number of the projection, taking it as gospel—when, in fact, it obscures a (wide) range of possibilities. For example, there’s a big difference between a projected ocean temperature rise of 1ºC with a margin of error of 0.05ºC and a projection of 1ºC with a margin of error of 5ºC.

**2) The “Far Future” Fallacy**

The further into the future one projects, the wider is that projection’s margin for error (because of the greater possibility for random occurrences), yet we treat these projections similarly to shorter-term projections. Classic examples come from literature: George Orwell’s _1984_ (published in 1949), for example, was far off in terms of the state of the world in the mid-Eighties.

_3) **The “Black Swan” Fallacy**_

We underestimate the randomness of the variables used in the projection—that is, we fail to understand that any part of the method used to determine the projection is susceptible to Black Swans. For an example, see the oil-price-projection example just above.

### Predictors = Liars

When leveling his critiques at financial, political, and security analysts—people who make their living from forecasting—Taleb often gets asked (snippily) to propose _his own_ predictions. In these situations, **Taleb freely admits _that he cannot forecast and that it would be irresponsible to attempt to_**.

In fact, he goes a step further: He encourages those who forecast uncritically—the “incompetent arrogants”—to get new jobs. To him, bad forecasters are either fools or liars and do more damage to society than criminals.

[

Previous

Exercise: Responding to Randomness

](https://www.shortform.com/app/book/the-black-swan/exercise-responding-to-randomness)

[

Next

Exercise: Learning the Limits of Prediction

](https://www.shortform.com/app/book/the-black-swan/exercise-learning-the-limits-of-prediction)